# cs6210Project4
MapReduce Infrastructure

# General Design
The Master::run function orchestrates the entire MapReduce process. Here's a breakdown of how it works step by step:
1. Initialization and Thread Creation:
    It starts by creating two background threads:
    - monitoring_thread: Executes the Master::monitor function, responsible for periodically checking the health of the worker nodes.
    - spawn_map_jobs: Executes the Master::poll_workers function, which listens for responses from workers after they complete a map or reduce task.
2. Waiting for Initial Monitor Setup:
    - The master waits for the initial monitoring cycle to complete. This is ensured by using a condition variable cv_monitor and a flag initial_monitor_job. The main thread pauses until the monitor thread finishes its first round of pings and pongs with the workers.
3. Map Phase:
    - The complete_all_shards flag is initialized to false, and a while loop continues until all input file shards are processed.
    - It iterates through the shards (representing the input file splits). For each shard:
        - It acquires a lock on the mutex m to safely access the ready_queue (which holds the IP addresses of available workers).
        - It waits on the condition variable cv until a worker is available in the ready_queue.
        - It retrieves the IP address of a ready worker from the front of the ready_queue and removes it.
        - It checks if the selected worker has been marked as dropped. If so, it decrements the loop counter i and continues to the next iteration to pick a different worker.
        - It records the assignment of the current shard index to the selected worker_ip in the worker2job map.
        - It releases the lock on m and notifies any waiting threads on cv.
        - It retrieves the WorkerClient associated with the selected worker_ip from the ip_addr2worker map.
        - It calls the send_mapper_job method on the WorkerClient to send the current file shard to the worker for processing. This includes the user_id and the num_partitions for the subsequent reduce phase.
    - After assigning all initial shards to workers, it calls schedule_monitor(). This function signals the monitoring thread to perform a check and then determines if the map phase is complete.
    - It checks if all shards have been processed (unfinished_shards.size() == 0) and if the map phase is marked as complete (map_complete). If both are true, it clears the shards vector and sets complete_all_shards to true to exit the loop.
    - If there are any unfinished_shards (due to worker failures), it clears the current shards vector and moves the unfinished_shards into it for reprocessing in the next iteration.
4. Waiting for Map Completion:
    - After the map phase loop finishes, it calls spawn_map_jobs.join() to wait for the poll_workers thread (which was handling map responses) to complete. This ensures all map responses have been processed.
5. Preparing for Reduce Phase:
    - It creates an idx2files unordered map to group the intermediate files generated by the map tasks based on their partition index. It parses the filenames to extract this index.
    - It populates the partitions vector with the unique partition indices extracted from the intermediate filenames.
6. Reduce Phase:
    - It creates another background thread spawn_reduce_jobs that also executes the Master::poll_workers function. This thread will now handle responses from the reduce workers.
    - The complete_all_partitions flag is initialized to false, and a while loop continues until all partitions are processed.
    - It iterates through the partitions. For each partition index:
        - It acquires a lock on the mutex m and waits on cv until a worker is available in the ready_queue.
        - It retrieves a ready worker's IP address.
        - It checks if the worker is dropped.
        - It records the assignment of the current partition index to the worker in worker2job.
        - It releases the lock and notifies waiting threads.
        - It retrieves the WorkerClient for the worker.
        - It retrieves the list of intermediate files associated with the current partition index from idx2files.
        - It calls send_reducer_job on the WorkerClient to send the partition index, output directory, and the list of relevant intermediate files to the worker for the reduce task.
    - After assigning all partitions, it calls schedule_monitor().
    - It checks if all partitions have been processed (unfinished_partitions.size() == 0) and if the reduce phase is complete (reduce_complete). If so, it clears the partitions vector and sets complete_all_partitions to true.
    - If there are any unfinished_partitions, it updates the partitions vector for reprocessing.
7. Waiting for Reduce Completion:
    - After the reduce phase loop, it calls spawn_reduce_jobs.join() to wait for the reduce response handling thread to finish.
8. Stopping the Monitoring Thread:
    - It acquires a lock on m_monitor.
    - It sets the stop_monitoring flag to true and signals the monitor thread using cv_monitor.notify_one() to exit its loop.
    - It also sets trigger_monitoring to true and notifies cv_monitor again to ensure the monitor thread wakes up and checks the stop_monitoring flag.
    - It releases the lock.
    - It calls monitoring_thread.join() to wait for the monitoring thread to terminate.
9. Cleaning Up Intermediate Files:
    - It prints a message indicating the deletion of intermediate files.
    - It iterates through the intermediate_files set and deletes each file.
    - It attempts to remove the "intermediate" directory. It handles the case where the directory is not empty (which shouldn't happen if all files were deleted) and other potential errors.

The worker::run is responsible for setting up the gRPC server and handling incoming requests for both map/reduce tasks and simple ping requests. It uses a ServerBuilder to:
1. Specify the listening ip_addr_port_ with insecure credentials.
2. Register the worker's service implementation (service_).
3. Create and assign separate completion queues (map_reduce_queue and ping_queue) for handling map/reduce and ping requests asynchronously.
4. Build and start the gRPC server (server_).
It then launches a separate thread to continuously process incoming ping requests via the process_ping_request function and enters a loop in the main thread (process_map_reduce_request) to continuously process map and reduce task requests.

# How to Handle Slow Worker
We set a timeout(defined in `const.h`) when sending a request to the worker process(`worker_client.cc`). And in the `poll_worker` of the master thread, when a request is timeout, we will drop the job assigned to it and direct it to another worker.

## Project Instructions

[Project Description](description.md)

[Code walk through](structure.md)

### How to setup the project
Same as project 3 instructions

You have two options for your setup: You can either use the existing configuration from Project 3, or you can pull a Docker image with all the necessary installations using the command:

```bash
docker pull dcchico/aos_project4
```

### NOTES

The data utilized for this assignment is derived from publicly accessible sources on the internet, including popular literature. It's important to note that the data may contain inappropriate language or content, and it's crucial to understand that the teaching team is not accountable for or able to regulate the presence of such material.

### IMPORTANT INSTRUCTIONS

Ensure that all intermediate files generated by map tasks are deleted once each map reduce task is completed. Neglecting to perform this action may lead to test failures on gradescope.
